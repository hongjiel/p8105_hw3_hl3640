---
title: "Homework 3"
author: "Hongjie Liu"
output: github_document
---


Load necessary packages for homework 3.

```{r loadpackages, message = FALSE}
library(tidyverse)
library(p8105.datasets)
library(scales)
library(ggridges)
library(gridExtra)
```


## Problem 1

Read the dataset `instacart`.

```{r p1_readdata}
data("instacart")
```

Here is a short description of the dataset:

* The variables of the dataset are ``r names(instacart)``.
* The dataset has `r nrow(instacart)` rows (number of observations) and `r ncol(instacart)` columns.
* Some variables' meaning: `order_dow` indicates the day of the week on which the order was placed; `order_hour_of_day` indicates the hour of the day on which the order was placed. `days_since_prior_order` indicates days since the last order, capped at 30, NA if `order_number = 1`; `product_name` indicates name of the product; the value of the variable `reordered` is 1 if this product has been ordered by this user in the past, 0 otherwise.

```{r p1_aisles}
aisles_df = instacart %>% 
  group_by(aisle) %>% 
  summarize(n_obs = n()) %>% 
  mutate(n_ranking = min_rank(desc(n_obs))) %>% 
  arrange(n_ranking)
```

There are `r nrow(aisles_df)` aisles, and the aisle "`r pull(aisles_df, aisle)[1]`" is the most items ordered from.

Make a plot that shows the number of items ordered in each aisle, limiting this to aisles with more than 10000 items ordered. Arrange aisles sensibly, and organize your plot so others can read it.

```{r p1_plot}
aisles_df %>% 
  filter(n_obs > 10000) %>% 
  ggplot(aes(x = reorder(aisle, -n_obs), y = n_obs)) +
  geom_point() +
  labs(
    x = "Aisles",
    y = "Number of Items Ordered"
  ) +
  scale_y_continuous(
    breaks = c(10000, 20000, 40000, 80000, 160000),
    trans = "sqrt"
  ) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))
```

Make a table showing the three most popular items in each of the aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”. Include the number of times each item is ordered in your table.

```{r p1_table1, message = FALSE}
instacart %>% 
  filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>% 
  group_by(aisle, product_name) %>% 
  summarize(n_obs = n()) %>% 
  filter(min_rank(desc(n_obs)) < 4) %>% 
  arrange(aisle, desc(n_obs)) %>% 
  knitr::kable(caption = "Table 1.1: three most popular items in each of the aisles", col.names = c("Aisle", "Item", "Number of Times Ordered"))
```

Make a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week; format this table for human readers (i.e. produce a 2 x 7 table).

```{r p1_table2, message = FALSE}
instacart %>% 
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>% 
  group_by(product_name, order_dow) %>% 
  summarize(mean_hour = mean(order_hour_of_day)) %>% 
  mutate(
    order_dow = as.character(order_dow),
    order_dow = 
      recode(
        order_dow, 
        "0" = "Sunday",
        "1" = "Monday",
        "2" = "Tuesday",
        "3" = "Wednesday",
        "4" = "Thursday",
        "5" = "Friday",
        "6" = "Saturday"
      )
  ) %>% 
  pivot_wider(
  names_from = "order_dow", 
  values_from = "mean_hour"
  ) %>% 
  rename(Item = product_name) %>% 
  knitr::kable(
    caption = "Table 1.2: mean hour of the day at which items are ordered on each day of the week",
    digit = 1
  )
```


## Problem 2


Load, tidy, and otherwise wrangle the data. The final dataset `accel_df` includes all originally observed variables and values; has useful variable names; includes a weekday vs weekend variable; and encodes data with reasonable variable classes.

```{r p2_readtidy, message = FALSE}
accel_df =
  read_csv("./data/accel_data.csv") %>% 
  janitor::clean_names() %>% 
  pivot_longer(
    cols = activity_1:activity_1440,
    names_to = "minute",
    names_prefix = "activity_",
    values_to = "activity_counts"
  ) %>% 
  mutate(
    weekday = ifelse(day %in% c("Saturday", "Sunday"), FALSE, TRUE),
    minute = as.numeric(minute),
    day = factor(day, levels = c("Sunday", "Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday"))
  )

accel_df
```

Here is a short description of the resulting dataset `accel_df`:

* The variables are ``r names(accel_df)``.
* The dataset has `r nrow(accel_df)` rows (number of observations) and `r ncol(accel_df)` columns.

Using the tidied dataset, aggregate across minutes to create a total activity variable for each day, and create a table showing these totals.

```{r p2_table, message = FALSE}
accel_df %>% 
  group_by(week, day) %>% 
  summarize(total_activity = sum(activity_counts)) %>% 
  pivot_wider(
    names_from = day,
    values_from = total_activity
  ) %>% 
  knitr::kable(caption = "Table 2.1: total activity for each day", digit = 0)
```

From the table, it seems that there are no apparent trends. Besides, 2 observations (the two Saturdays in week 4 and week 5) have `total_activity = 1440` with `activity_counts = 1` for each minute, and they are probably caused by mistake.

Make a single-panel plot that shows the 24-hour activity time courses for each day and use color to indicate day of the week.

```{r p2_plot, message = FALSE}
accel_df %>% 
  mutate(
    time =
      paste(
        as.character(sprintf("%02d", (minute - 1) %/% 60)),
        as.character(sprintf("%02d", (minute - 1) %% 60)),
        sep = ":"
      ) %>% 
      as.POSIXct(format = "%H:%M", tz = "America/New_York")
  ) %>% 
  ggplot(aes(x = time, y = activity_counts, color = day)) +
  geom_line(alpha = .25) +
  geom_smooth(se = FALSE) +
  scale_x_datetime(labels = date_format("%I:%M %p", tz = "America/New_York")) +
  labs(x = "Time", y = "Activity Counts", color = "Day of the Week") +
  theme(legend.position = "bottom")
```

It seems that activity counts peak on Friday at around 9 pm and on Sunday at around 10:30 am, and are relatively low between midnight and dawn (probably sleeping hours).


## Problem 3

Read the dataset `ny_noaa`.

```{r p3_readdata}
data("ny_noaa")
```

Here is a description of the dataset `ny_noaa`:

* The variables of the dataset are ``r names(ny_noaa)``.
* The dataset has `r nrow(ny_noaa)` rows (number of observations) and `r ncol(ny_noaa)` columns.
* `id` indicates weather station ID; `prcp` indicates precipitation; `snow` indicates snowfall; `snwd` indicates snow depth; `tmax` and `tmin` indicate max and min temperature respectively.
* `r  round(nrow(drop_na(ny_noaa)) / nrow(ny_noaa), 3) * 100`% of the observations contain at least one missing value.

Do some data cleaning. Create separate variables for year, month, and day. Ensure observations for temperature, precipitation, and snowfall are given in reasonable units. For temperature, the unit is degree Celsius. For precipitation, the unit is millimeter. For snowfall and snow depth, the units are centimeter. (we keep the `snow` variable and add a new variable `snow_cm` for unit changes)

```{r p3_readtidy}
noaa_df = ny_noaa %>% 
  separate(date, into = c("year", "month", "day"), sep = "-", convert = TRUE) %>% 
  mutate(
    month = month.name[month],
    tmax = as.numeric(tmax),
    tmin = as.numeric(tmin),
    prcp = prcp / 10,
    snow_cm = snow / 10,
    snwd = snwd / 10,
    tmax = tmax / 10,
    tmin = tmin / 10,
  )

mc_value =
  noaa_df %>% 
  drop_na() %>% 
  group_by(snow) %>% 
  summarize(n_obs = n()) %>% 
  mutate(n_ranking = min_rank(desc(n_obs))) %>% 
  arrange(n_ranking) %>% 
  select(snow)
```

For snowfall, the most commonly observed value is `r mc_value[1, 1]`. I think it's because most days of the year do not snow.

Make a two-panel plot showing the average max temperature in January and in July in each station across years.

```{r p3_plot1, message = FALSE, warning = FALSE}
noaa_df %>% 
  filter(month %in% c("January", "July")) %>% 
  group_by(month, id, year) %>% 
  summarize(mean_tmax = mean(tmax)) %>% 
  drop_na() %>% 
  ggplot(aes(x = year, y = mean_tmax, color = id)) +
  geom_line(alpha = .5) +
  facet_wrap(. ~ month, ncol = 1,  scales = "free_y") +
  labs(x = "Year", y = "Average Max Temperature (°C)") +
  theme(legend.position = "none")
```

From the plot, it seems that there is no apparent increasing or decreasing trend in average max temperature in January and in July in each station. There is an apparent outlier, which is the average max temperature in July 1988 in one station.

Make a two-panel plot showing (i) `tmax` vs `tmin` for the full dataset (note that a scatterplot may not be the best option); and (ii) make a plot showing the distribution of snowfall values greater than 0 mm and less than 100 mm separately by year. 

```{r p3_plot2, message = FALSE , warning = FALSE}
plot_a =
  ggplot(noaa_df, aes(x = tmax, y = tmin)) +
  geom_hex() +
  labs(x = "Max Temperature (°C)", y = "Min Temperature (°C)") +
  theme(legend.position = "none")

plot_b =
  noaa_df %>% 
  filter(
    snow > 0,
    snow < 100
  ) %>% 
  mutate(year = as.character(year)) %>% 
  ggplot(aes(x = snow, y = year)) +
  geom_density_ridges(scale = .85) +
  scale_y_discrete(limits = rev) +
  labs(x = "Snowfall (mm)", y = "Year")

grid.arrange(plot_a, plot_b, ncol = 2)
```
